{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db60baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "mbti_df = pd.read_csv('mbti_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d43947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Text normalization \n",
    "# (i) convert to lowercase;\n",
    "# (ii) remove url; \n",
    "# (iii) remove numbers;\n",
    "# (iv) remove non-alphanumeric characters (punctuation, special characters);\n",
    "# (v) remove underscores and signs;\n",
    "# (vi) replace multiple spaces with single spaces;\n",
    "# (vii) remove stopwords; \n",
    "# (viii) remove one-letter words;\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    pattern = re.compile(r'https?://[a-zA-Z0-9./-]*/[a-zA-Z0-9?=_.]*[_0-9.a-zA-Z/-]*')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'[0-9]')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'\\W+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'[_+]')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text).strip()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text = \" \".join([w for w in text.split() if w not in stop_words])\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lemmatization\n",
    "# (i) use NLTK's lemmatizer\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5656c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select classification dimension (16-class classification or binary classification on single dimension)\n",
    "def select_classification_dimension(df, num=0):\n",
    "    if num == 16:\n",
    "        return df['type']\n",
    "    elif 1 <= num <= 4:\n",
    "        return df['type'].str[num-1]\n",
    "    else:\n",
    "        print(\"selection error of classification dimension!\")\n",
    "        return df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086bb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Label encoding\n",
    "# (i) creates an array corresponding to the type labels.\n",
    "def encode_labels(column):\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(column)\n",
    "    mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "    print(f\"Label encoding mapping: {mapping}\")\n",
    "    print(f\"Encoded label examples: {y[:5]}\")\n",
    "    print(f\"Unique encoded labels: {np.unique(y)}\")\n",
    "    return y, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6014ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df[\"posts\"] = mbti_df[\"posts\"].str.lower()       #converts text in posts to lowercase as it is preferred in nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9877527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mbti_df)):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  pattern = re.compile(r'https?://[a-zA-Z0-9./-]*/[a-zA-Z0-9?=_.]*[_0-9.a-zA-Z/-]*')    #to match url links present in the post\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                                            #to replace that url link with space\n",
    "  mbti_df._set_value(i, 'posts',post_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f236c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mbti_df)):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  pattern = re.compile(r'[0-9]')                                    #to match numbers from 0 to 9\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                        #to replace them with space\n",
    "  pattern = re.compile('\\W+')                                       #to match alphanumeric characters\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                        #to replace them with space\n",
    "  pattern = re.compile(r'[_+]')\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)\n",
    "  mbti_df._set_value(i, 'posts',post_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mbti_df)):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  pattern = re.compile('\\s+')                                     #to match multiple whitespaces\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                      #to replace them with single whitespace\n",
    "  mbti_df._set_value(i, 'posts', post_temp)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9390486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enfp and intj moments sportscenter not top ten plays pranks what has been the most life changing experience in your life on repeat for most of today may the perc experience immerse you the last thing my infj friend posted on his facebook before committing suicide the next day rest in peace hello enfj sorry to hear of your distress it s only natural for a relationship to not be perfection all the time in every moment of existence try to figure the hard times as times of growth as welcome and stuff game set match prozac wellbrutin at least thirty minutes of moving your legs and i don t mean moving them while sitting in your same desk chair weed in moderation maybe try edibles as a healthier alternative basically come up with three items you ve determined that each type or whichever types you want to do would more than likely use given each types cognitive functions and whatnot when left by all things in moderation sims is indeed a video game and a good one at that note a good one at that is somewhat subjective in that i am not completely promoting the death of any given sim dear enfp what were your favorite video games growing up and what are your now current favorite video games cool it appears to be too late sad there s someone out there for everyone wait i thought confidence was a good thing i just cherish the time of solitude b c i revel within my inner world more whereas most other time i d be workin just enjoy the me time while you can don t worry people will always be around to yo entp ladies if you re into a complimentary personality well hey when your main social outlet is xbox live conversations and even then you verbally fatigue quickly i really dig the part from to banned because this thread requires it of me get high in backyard roast and eat marshmellows in backyard while conversing over something intellectual followed by massages and kisses banned for too many b s in that sentence how could you think of the b banned for watching movies in the corner with the dunces banned because health class clearly taught you nothing about peer pressure banned for a whole host of reasons two baby deer on left and right munching on a beetle in the middle using their own blood two cavemen diary today s latest happenings on their designated cave diary wall i see it as a pokemon world an infj society everyone becomes an optimist not all artists are artists because they draw it s the idea that counts in forming something of your own like a signature welcome to the robot ranks person who downed my self esteem cuz i m not an avid signature artist like herself proud banned for taking all the room under my bed ya gotta learn to share with the roaches banned for being too much of a thundering grumbling kind of storm yep ahh old high school music i haven t heard in ages i failed a public speaking class a few years ago and i ve sort of learned what i could do better were i to be in that position again a big part of my failure was just overloading myself with too i like this person s mentality he s a confirmed intj by the way move to the denver area and start a new life for myself \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf46323",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = stopwords.words(\"english\")\n",
    "for i in range(mbti_df.shape[0]):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  post_temp=\" \".join([w for w in post_temp.split(' ') if w not in remove_words])    #to remove stopwords\n",
    "  mbti_df._set_value(i, 'posts', post_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c006c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enfp intj moments sportscenter top ten plays pranks life changing experience life repeat today may perc experience immerse last thing infj friend posted facebook committing suicide next day rest peace hello enfj sorry hear distress natural relationship perfection time every moment existence try figure hard times times growth welcome stuff game set match prozac wellbrutin least thirty minutes moving legs mean moving sitting desk chair weed moderation maybe try edibles healthier alternative basically come three items determined type whichever types want would likely use given types cognitive functions whatnot left things moderation sims indeed video game good one note good one somewhat subjective completely promoting death given sim dear enfp favorite video games growing current favorite video games cool appears late sad someone everyone wait thought confidence good thing cherish time solitude b c revel within inner world whereas time workin enjoy time worry people always around yo entp ladies complimentary personality well hey main social outlet xbox live conversations even verbally fatigue quickly really dig part banned thread requires get high backyard roast eat marshmellows backyard conversing something intellectual followed massages kisses banned many b sentence could think b banned watching movies corner dunces banned health class clearly taught nothing peer pressure banned whole host reasons two baby deer left right munching beetle middle using blood two cavemen diary today latest happenings designated cave diary wall see pokemon world infj society everyone becomes optimist artists artists draw idea counts forming something like signature welcome robot ranks person downed self esteem cuz avid signature artist like proud banned taking room bed ya gotta learn share roaches banned much thundering grumbling kind storm yep ahh old high school music heard ages failed public speaking class years ago sort learned could better position big part failure overloading like person mentality confirmed intj way move denver area start new life \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/meitongliu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c064be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enfp intj moment sportscenter top ten play prank life changing experience life repeat today may perc experience immerse last thing infj friend posted facebook committing suicide next day rest peace hello enfj sorry hear distress natural relationship perfection time every moment existence try figure hard time time growth welcome stuff game set match prozac wellbrutin least thirty minute moving leg mean moving sitting desk chair weed moderation maybe try edible healthier alternative basically come three item determined type whichever type want would likely use given type cognitive function whatnot left thing moderation sims indeed video game good one note good one somewhat subjective completely promoting death given sim dear enfp favorite video game growing current favorite video game cool appears late sad someone everyone wait thought confidence good thing cherish time solitude b c revel within inner world whereas time workin enjoy time worry people always around yo entp lady complimentary personality well hey main social outlet xbox live conversation even verbally fatigue quickly really dig part banned thread requires get high backyard roast eat marshmellows backyard conversing something intellectual followed massage kiss banned many b sentence could think b banned watching movie corner dunce banned health class clearly taught nothing peer pressure banned whole host reason two baby deer left right munching beetle middle using blood two caveman diary today latest happening designated cave diary wall see pokemon world infj society everyone becomes optimist artist artist draw idea count forming something like signature welcome robot rank person downed self esteem cuz avid signature artist like proud banned taking room bed ya gotta learn share roach banned much thundering grumbling kind storm yep ahh old high school music heard age failed public speaking class year ago sort learned could better position big part failure overloading like person mentality confirmed intj way move denver area start new life \n"
     ]
    }
   ],
   "source": [
    "for i in range(mbti_df.shape[0]):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  post_temp=\" \".join([lemmatizer.lemmatize(w) for w in post_temp.split(' ')])   #to implement lemmetization i.e. to group together different forms of a word\n",
    "  mbti_df._set_value(i, 'posts', post_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "700b3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts\n",
      "0     INFJ   enfp intj moment sportscenter top ten play pr...\n",
      "1     ENTP   finding lack post alarming sex boring positio...\n",
      "2     INTP   good one course say know blessing curse absol...\n",
      "3     INTJ   dear intp enjoyed conversation day esoteric g...\n",
      "4     ENTJ   fired another silly misconception approaching...\n",
      "...    ...                                                ...\n",
      "8670  ISFP   ixfp always think cat fi doms reason especial...\n",
      "8671  ENFP   thread already exists someplace else post hec...\n",
      "8672  INTP   many question thing would take purple pill pi...\n",
      "8673  INFP   conflicted right come wanting child honestly ...\n",
      "8674  INFP   long since personalitycafe although seem chan...\n",
      "\n",
      "[8675 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mbti_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f00092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts\n",
      "7814  INFP   macona depends big family extroverted people ...\n",
      "2233  ENFJ   blodsmak sveltihel brilliant episode regenera...\n",
      "7261  INFJ   heylena lol compliment accepted thank jeesh f...\n",
      "7794  INFJ   pac right rocket coffin like packed warhead r...\n",
      "2950  INTJ   title thread misleading mention world dominat...\n",
      "...    ...                                                ...\n",
      "2006  INTJ   one sentence restrictive accurately portray d...\n",
      "7137  ISTJ   wanted like odd hybrid dr james wilson house ...\n",
      "6091  ENTP   took cognitive process test got cognitive pro...\n",
      "2997  INFJ   get caught fantacy relationship better forget...\n",
      "5458  ENTJ   doll love movie listed make think tritype one...\n",
      "\n",
      "[1735 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data=train_test_split(mbti_df,test_size=0.2,random_state=42,stratify=mbti_df.type)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0789985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer( max_features=5000,stop_words='english')\n",
    "vectorizer.fit(train_data.posts)\n",
    "train_post=vectorizer.transform(train_data.posts).toarray()\n",
    "test_post=vectorizer.transform(test_data.posts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3338d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_encoder=LabelEncoder()\n",
    "train_target=target_encoder.fit_transform(train_data.type)\n",
    "test_target=target_encoder.fit_transform(test_data.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84306028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/meitongliu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def extract_stylometric_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    num_chars = len(text)\n",
    "    num_exclamations = text.count('!')\n",
    "    num_questions = text.count('?')\n",
    "    num_uppercase_words = sum(1 for w in words if w.isupper())\n",
    "    lexical_diversity = len(set(words)) / num_words if num_words > 0 else 0\n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return [\n",
    "        num_sentences,\n",
    "        num_words,\n",
    "        num_chars,\n",
    "        avg_word_length,\n",
    "        avg_sentence_length,\n",
    "        num_exclamations,\n",
    "        num_questions,\n",
    "        num_uppercase_words,\n",
    "        lexical_diversity\n",
    "    ]\n",
    "\n",
    "# Apply to both train and test\n",
    "train_stylo = train_data[\"posts\"].apply(extract_stylometric_features).tolist()\n",
    "test_stylo = test_data[\"posts\"].apply(extract_stylometric_features).tolist()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_stylo_np = np.array(train_stylo)\n",
    "test_stylo_np = np.array(test_stylo)\n",
    "\n",
    "X_train_combined = np.hstack([train_post, train_stylo_np])\n",
    "X_test_combined = np.hstack([test_post, test_stylo_np])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f21d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meitongliu/miniconda/anaconda3/envs/psy/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define MBTI traits\n",
    "traits = ['I', 'E', 'N', 'S', 'T', 'F', 'J', 'P']\n",
    "\n",
    "# 1: Group posts by trait to build trait_keywords\n",
    "trait_groups = {trait: [] for trait in traits}\n",
    "for i, row in mbti_df.iterrows():\n",
    "    for t in row['type']:\n",
    "        if t in traits:\n",
    "            trait_groups[t].append(row['posts'])\n",
    "\n",
    "# 2: Extract top TF-IDF keywords for each trait\n",
    "def clean_tokenizer(text):\n",
    "    custom_stopwords = set([\n",
    "        'like', 'just', 'don', 'com', 'http', 'www', 'youtube', 'watch', 'infp', \n",
    "        'intj', 'infj', 'intp', 'enfp', 'entp', 'type', 'https', 've', 'istp'\n",
    "    ])\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    return [t for t in tokens if t not in custom_stopwords]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "trait_keywords = {}\n",
    "top_k = 20\n",
    "\n",
    "for trait, posts in trait_groups.items():\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=clean_tokenizer,\n",
    "        stop_words='english',\n",
    "        max_features=1000\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform(posts)\n",
    "    mean_scores = tfidf.mean(axis=0).A1\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    top_indices = mean_scores.argsort()[::-1][:top_k]\n",
    "    top_words = [vocab[i] for i in top_indices]\n",
    "    trait_keywords[trait] = top_words\n",
    "\n",
    "# 3: Build transition matrix\n",
    "co_matrix = np.zeros((8, 8))\n",
    "trait_index = {t: i for i, t in enumerate(traits)}\n",
    "for mbti in mbti_df['type']:\n",
    "    chars = list(mbti)\n",
    "    for t1 in chars:\n",
    "        for t2 in chars:\n",
    "            if t1 != t2:\n",
    "                i, j = trait_index[t1], trait_index[t2]\n",
    "                co_matrix[i][j] += 1\n",
    "row_sums = co_matrix.sum(axis=1, keepdims=True)\n",
    "transition_matrix = co_matrix / row_sums\n",
    "\n",
    "# 4: Define vector extraction function\n",
    "def extract_trait_vector(text, trait_keywords, use_transition=False, transition_matrix=None, normalize=True):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    base_vector = np.array([\n",
    "        sum(word_counts.get(w, 0) for w in trait_keywords[t]) for t in traits\n",
    "    ])\n",
    "    if not use_transition:\n",
    "        return base_vector\n",
    "    if base_vector.sum() == 0:\n",
    "        return np.zeros(8)\n",
    "    if normalize:\n",
    "        base_vector = base_vector / base_vector.sum()\n",
    "    return np.dot(base_vector, transition_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27eec017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector (using transition matrix)\n",
    "train_trait_vector = np.vstack(\n",
    "    train_data['posts'].apply(lambda x: extract_trait_vector(x, trait_keywords, use_transition=True, transition_matrix=transition_matrix, normalize=True))\n",
    ")\n",
    "\n",
    "test_trait_vector = np.vstack(\n",
    "    test_data['posts'].apply(lambda x: extract_trait_vector(x, trait_keywords, use_transition=True, transition_matrix=transition_matrix, normalize=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a91c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = np.hstack([train_post, train_trait_vector])\n",
    "test_combined = np.hstack([test_post, test_trait_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb2634f-cbbc-441c-8d54-5483ef5ee458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.12994064 0.09393819 0.15488145]\n",
      " [0.         0.         0.         ... 0.13009538 0.09352525 0.15576653]\n",
      " [0.         0.         0.         ... 0.1296437  0.09333251 0.15491105]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.13133614 0.09390271 0.15609729]\n",
      " [0.         0.         0.         ... 0.12987942 0.09388824 0.15531304]\n",
      " [0.         0.         0.         ... 0.12961804 0.09392052 0.15519209]]\n"
     ]
    }
   ],
   "source": [
    "print(train_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68204fcf-23ab-48e6-ab76-c2bbf6d1f7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
