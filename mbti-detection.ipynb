{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1db60baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "mbti_df = pd.read_csv('mbti_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00d43947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Text normalization \n",
    "# (i) convert to lowercase;\n",
    "# (ii) remove url; \n",
    "# (iii) remove numbers;\n",
    "# (iv) remove non-alphanumeric characters (punctuation, special characters);\n",
    "# (v) remove underscores and signs;\n",
    "# (vi) replace multiple spaces with single spaces;\n",
    "# (vii) remove stopwords; \n",
    "# (viii) remove one-letter words;\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    pattern = re.compile(r'https?://[a-zA-Z0-9./-]*/[a-zA-Z0-9?=_.]*[_0-9.a-zA-Z/-]*')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'[0-9]')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'\\W+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'[_+]')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text).strip()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text = \" \".join([w for w in text.split() if w not in stop_words])\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e9ab404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lemmatization\n",
    "# (i) use NLTK's lemmatizer\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5656c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select classification dimension (16-class classification or binary classification on single dimension)\n",
    "def select_classification_dimension(df, num=0):\n",
    "    if num == 16:\n",
    "        return df['type']\n",
    "    elif 1 <= num <= 4:\n",
    "        return df['type'].str[num-1]\n",
    "    else:\n",
    "        print(\"selection error of classification dimension!\")\n",
    "        return df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "086bb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Label encoding\n",
    "# (i) creates an array corresponding to the type labels.\n",
    "def encode_labels(column):\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(column)\n",
    "    mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "    print(f\"Label encoding mapping: {mapping}\")\n",
    "    print(f\"Encoded label examples: {y[:5]}\")\n",
    "    print(f\"Unique encoded labels: {np.unique(y)}\")\n",
    "    return y, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6014ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_df[\"posts\"] = mbti_df[\"posts\"].str.lower()       #converts text in posts to lowercase as it is preferred in nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9877527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mbti_df)):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  pattern = re.compile(r'https?://[a-zA-Z0-9./-]*/[a-zA-Z0-9?=_.]*[_0-9.a-zA-Z/-]*')    #to match url links present in the post\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                                            #to replace that url link with space\n",
    "  mbti_df._set_value(i, 'posts',post_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f236c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mbti_df)):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  pattern = re.compile(r'[0-9]')                                    #to match numbers from 0 to 9\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                        #to replace them with space\n",
    "  pattern = re.compile('\\W+')                                       #to match alphanumeric characters\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                        #to replace them with space\n",
    "  pattern = re.compile(r'[_+]')\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)\n",
    "  mbti_df._set_value(i, 'posts',post_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19e723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mbti_df)):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  pattern = re.compile('\\s+')                                     #to match multiple whitespaces\n",
    "  post_temp= re.sub(pattern, ' ', post_temp)                      #to replace them with single whitespace\n",
    "  mbti_df._set_value(i, 'posts', post_temp)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9390486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acf46323",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = stopwords.words(\"english\")\n",
    "for i in range(mbti_df.shape[0]):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  post_temp=\" \".join([w for w in post_temp.split(' ') if w not in remove_words])    #to remove stopwords\n",
    "  mbti_df._set_value(i, 'posts', post_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c006c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16c064be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mbti_df.shape[0]):\n",
    "  post_temp=mbti_df._get_value(i, 'posts')\n",
    "  post_temp=\" \".join([lemmatizer.lemmatize(w) for w in post_temp.split(' ')])   #to implement lemmetization i.e. to group together different forms of a word\n",
    "  mbti_df._set_value(i, 'posts', post_temp)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "700b3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts\n",
      "0     INFJ   enfp intj moment sportscenter top ten play pr...\n",
      "1     ENTP   finding lack post alarming sex boring positio...\n",
      "2     INTP   good one course say know blessing curse absol...\n",
      "3     INTJ   dear intp enjoyed conversation day esoteric g...\n",
      "4     ENTJ   fired another silly misconception approaching...\n",
      "...    ...                                                ...\n",
      "8670  ISFP   ixfp always think cat fi doms reason especial...\n",
      "8671  ENFP   thread already exists someplace else post hec...\n",
      "8672  INTP   many question thing would take purple pill pi...\n",
      "8673  INFP   conflicted right come wanting child honestly ...\n",
      "8674  INFP   long since personalitycafe although seem chan...\n",
      "\n",
      "[8675 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(mbti_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3f00092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts\n",
      "7814  INFP   macona depends big family extroverted people ...\n",
      "2233  ENFJ   blodsmak sveltihel brilliant episode regenera...\n",
      "7261  INFJ   heylena lol compliment accepted thank jeesh f...\n",
      "7794  INFJ   pac right rocket coffin like packed warhead r...\n",
      "2950  INTJ   title thread misleading mention world dominat...\n",
      "...    ...                                                ...\n",
      "2006  INTJ   one sentence restrictive accurately portray d...\n",
      "7137  ISTJ   wanted like odd hybrid dr james wilson house ...\n",
      "6091  ENTP   took cognitive process test got cognitive pro...\n",
      "2997  INFJ   get caught fantacy relationship better forget...\n",
      "5458  ENTJ   doll love movie listed make think tritype one...\n",
      "\n",
      "[1735 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data=train_test_split(mbti_df,test_size=0.2,random_state=42,stratify=mbti_df.type)\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0789985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer( max_features=5000,stop_words='english')\n",
    "vectorizer.fit(train_data.posts)\n",
    "train_post=vectorizer.transform(train_data.posts).toarray()\n",
    "test_post=vectorizer.transform(test_data.posts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3338d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_encoder=LabelEncoder()\n",
    "train_target=target_encoder.fit_transform(train_data.type)\n",
    "test_target=target_encoder.fit_transform(test_data.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84306028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extract_stylometric_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    num_chars = len(text)\n",
    "    num_exclamations = text.count('!')\n",
    "    num_questions = text.count('?')\n",
    "    num_uppercase_words = sum(1 for w in words if w.isupper())\n",
    "    lexical_diversity = len(set(words)) / num_words if num_words > 0 else 0\n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return [\n",
    "        num_sentences,\n",
    "        num_words,\n",
    "        num_chars,\n",
    "        avg_word_length,\n",
    "        avg_sentence_length,\n",
    "        num_exclamations,\n",
    "        num_questions,\n",
    "        num_uppercase_words,\n",
    "        lexical_diversity\n",
    "    ]\n",
    "\n",
    "# Apply to both train and test\n",
    "train_stylo = train_data[\"posts\"].apply(extract_stylometric_features).tolist()\n",
    "test_stylo = test_data[\"posts\"].apply(extract_stylometric_features).tolist()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_stylo_np = np.array(train_stylo)\n",
    "test_stylo_np = np.array(test_stylo)\n",
    "\n",
    "X_train_combined = np.hstack([train_post, train_stylo_np])\n",
    "X_test_combined = np.hstack([test_post, test_stylo_np])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f21d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define MBTI traits\n",
    "traits = ['I', 'E', 'N', 'S', 'T', 'F', 'J', 'P']\n",
    "\n",
    "# 1: Group posts by trait to build trait_keywords\n",
    "trait_groups = {trait: [] for trait in traits}\n",
    "for i, row in mbti_df.iterrows():\n",
    "    for t in row['type']:\n",
    "        if t in traits:\n",
    "            trait_groups[t].append(row['posts'])\n",
    "\n",
    "# 2: Extract top TF-IDF keywords for each trait\n",
    "def clean_tokenizer(text):\n",
    "    custom_stopwords = set([\n",
    "        'like', 'just', 'don', 'com', 'http', 'www', 'youtube', 'watch', 'infp', \n",
    "        'intj', 'infj', 'intp', 'enfp', 'entp', 'type', 'https', 've', 'istp'\n",
    "    ])\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    return [t for t in tokens if t not in custom_stopwords]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "trait_keywords = {}\n",
    "top_k = 20\n",
    "\n",
    "for trait, posts in trait_groups.items():\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=clean_tokenizer,\n",
    "        stop_words='english',\n",
    "        max_features=1000\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform(posts)\n",
    "    mean_scores = tfidf.mean(axis=0).A1\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    top_indices = mean_scores.argsort()[::-1][:top_k]\n",
    "    top_words = [vocab[i] for i in top_indices]\n",
    "    trait_keywords[trait] = top_words\n",
    "\n",
    "# 3: Build transition matrix\n",
    "co_matrix = np.zeros((8, 8))\n",
    "trait_index = {t: i for i, t in enumerate(traits)}\n",
    "for mbti in mbti_df['type']:\n",
    "    chars = list(mbti)\n",
    "    for t1 in chars:\n",
    "        for t2 in chars:\n",
    "            if t1 != t2:\n",
    "                i, j = trait_index[t1], trait_index[t2]\n",
    "                co_matrix[i][j] += 1\n",
    "row_sums = co_matrix.sum(axis=1, keepdims=True)\n",
    "transition_matrix = co_matrix / row_sums\n",
    "\n",
    "# 4: Define vector extraction function\n",
    "def extract_trait_vector(text, trait_keywords, use_transition=False, transition_matrix=None, normalize=True):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    base_vector = np.array([\n",
    "        sum(word_counts.get(w, 0) for w in trait_keywords[t]) for t in traits\n",
    "    ])\n",
    "    if not use_transition:\n",
    "        return base_vector\n",
    "    if base_vector.sum() == 0:\n",
    "        return np.zeros(8)\n",
    "    if normalize:\n",
    "        base_vector = base_vector / base_vector.sum()\n",
    "    return np.dot(base_vector, transition_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27eec017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector (using transition matrix)\n",
    "train_trait_vector = np.vstack(\n",
    "    train_data['posts'].apply(lambda x: extract_trait_vector(x, trait_keywords, use_transition=True, transition_matrix=transition_matrix, normalize=True))\n",
    ")\n",
    "\n",
    "test_trait_vector = np.vstack(\n",
    "    test_data['posts'].apply(lambda x: extract_trait_vector(x, trait_keywords, use_transition=True, transition_matrix=transition_matrix, normalize=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a91c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = np.hstack([train_post, train_trait_vector])\n",
    "test_combined = np.hstack([test_post, test_trait_vector])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
